\documentclass[11pt,letterpaper]{article}                  % Define document class

%\usepackage{amsfonts} % For \mathbb command
%\usepackage[top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage{amsmath} % for align
\usepackage{mathtools, cancel, xfrac} % for lVert
%\usepackage{graphicx} % for charts

% Set path
\newcommand{\path}{Preamble}

% Text packages
\input{"\path/packText.tex"}
\setlength\parindent{0pt}                                  % No indentation for the whole document

% Figure/table packages
\input{"\path/packFigure.tex"}

% Math packages
\input{"\path/packMath.tex"}

% Graphics packages
\input{"\path/packGraph.tex"}

\allowdisplaybreaks % so that aligned equations can be broken across pages

%% Convenient math abbreviations
\newcommand*\diff{\mathop{}\!d} % nicely formatted integral dx
%\newcommand{\E}{\mathrm{E}} % Expectation operator
\newcommand{\Var}{\mathrm{V}} % Variance operator
\newcommand{\Cov}{\mathrm{Cov}} % Covariance operator
%\newtheorem{problem}{Problem}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert} % For Euclidean norm


% Title
\title{Problem Set 2 \\ \medskip \Large{Econometrics III}}
\author{\Large Jackson Bunting, Attila Gyetvai, Peter Horvath, Leonardo Salim Saker Chaves}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle
\section{Consistency of M-Estimation} 


%1.1
\begin{problem}(Median) Let Y be a $\mathbb{R}$-valued random variable with probability density $f_Y(\cdot)$. Let $\theta_0$ be the median of Y (i.e. $\theta_0 = \inf \{y: \mathbb{P}(Y \leq y) \geq 0.5 \}).$ Suppose that the function $f_Y$ is continuous and $f_Y(\theta_0) > 0$. \\
	(a) Let $Q(\theta) = \mathbb{E}|Y - \theta|.$ Compute $Q'(\theta)$ and $Q''(\theta)$.
	\\
	(b) Show that $Q(\theta)$ is uniquely minimized at $\theta_0$. \\
	
	\textbf{Solution:}  \\
	
	\paragraph*{a} 
	\begin{align*}
		Q(\theta) &= \int_{\theta}^{\infty} (y - \theta) f_Y(y) \diff y + \int_{-\infty}^{ \theta} (y - \theta) f_Y(y) \diff y, \\
		\text{ therefore by Leibniz rule we can get}\\
		Q'(\theta) &= \int_{\theta}^{\infty} (-f_Y(y)) \diff y - (\theta - \theta) + \int_{-\infty}^{ \theta} f_Y(y) \diff y +  (\theta - \theta) \\ 
		&= - (1 - F_Y(\theta)) + F_Y(\theta) = 2 F_Y(\theta) - 1 \text{ and }\\
      	Q''(\theta) &= 2 f_Y(\theta). \\
		\end{align*} \\
		\paragraph*{b} Since $\theta_0$ is the median of Y, the function $f_Y$ is continuous and $f_Y(\theta_0) > 0$, $2 F_Y(\theta_0) - 1 = 0$ and $2 f_Y(\theta_0) > 0$, that is, $Q(\theta)$ is uniquely minimized at $\theta_0$. 

\end{problem}

\bigskip
%1.2
\begin{problem}
If $\theta \neq 0$ and $a \in \mathbb{R}$, then
$\mathrm{P}(\theta' X = a) < 1$
\paragraph*{a}
$\theta \in \mathbb{R}^2$ and $a \in \mathbb{R}$. Suppose
$\mathrm{P}(X'\theta = a) = 1$. Negating the above condition gives
$\theta = 0$. And then $a = 0$.

\paragraph*{b}
$\theta_1,\theta_2 \in \mathbb{R}^2$ and $a_1,a_2 \in
\mathbb{R}$. Suppose $1 = \mathrm{P}(X'\theta_1 + a_1 = X'\theta_2 +
a_2) = \mathrm{P}\left(X'(\theta_2-\theta_1) = (a_1-a_2)\right)$. Part
a gives $\theta_1 = \theta_2$ and $a_1=a_2$.
\end{problem}

\bigskip
%1.3
\begin{problem}[Conditional Median and Quantile Regression]
Let $Y$ be an $\mathbb{R}$-valued random variable and $X$ be an $\mathbb{R}^d$-valued random variable.
Let $f_{Y|X=x} (y)$ be the conditional probability density of $Y$ given $X=x$ and $\theta_0$ be the conditional median of $Y$ given $X=x$.
Suppose that for any $x \in \mathbb{R}^d$, $f_{Y|X=x} (y)$ is continuous in $y$ and $f_{Y|X=x} (\theta_0) > 0$.
\begin{enumerate}[(a)]
	\item 
	Show that $\E [|Y - f(X)|] = \E [ \E_X [|Y - f(X)|]]$ for any measurable function $f$, where $\E_X$ is the conditional expectation operator. \\

	\textbf{Solution:}
	\begin{align*}
		\E \left[ \E_X \left[ |Y - f(X)| \right] \right] &= \int \int \E_X \left[ |Y - f(X)| \right] f_{Y,X} (y,x) dx \, dy\\
		&= \int \E_X \left[ |Y - f(X)| \right] f_X (x) dx \\
		&= \int \int |y - f(x)| f_{Y | X=x} (y) dy f_X (x) dx \\
		&= \int \int |y - f(x)| f_{Y | X=x} (y) f_X (x) dy \, dx \\
		&= \int \int |y - f(x)| f_{Y, X} (y,x) dy \, dx \\
		&= \E \left[ |Y - f(X)| \right]
	\end{align*}
	The second line follows from the fact that the inner conditional expectation is a function of $x$ only.

	\item 
	Show that $\E [|Y - f(X)|]$ is uniquely minimized when we take the function $f(\cdot)$ to be $\theta_0 (\cdot)$.
	\\

	\textbf{Solution:}
	Let $f_{Y|X=x} (y) = f_Z (y)$.
	Since the conditional median is unique, the result follows from Problem 1.1 (b).

	\item 
	Suppose that $\theta_0(x) = \alpha_0 + \beta_0'x$.
	Also suppose that the distribution of $X$ does not collapse to hyperplanes.
	Show that $\E \left| Y - \alpha - \beta'X \right|$ is uniquely minimized at $\alpha = \alpha_0$, $\beta = \beta_0$.
	\\

	\textbf{Solution:} Let $f(x) = \alpha + \beta' x$.
	From Part (b), we have a unique function.
	Since we assume linearity, it must be the case that $f(x)$ is linear.
	Now consider two minimizers $f(x)$ and $\theta_0(x)$.
	Then $\Pr (X'(\beta-\beta_0) = (\alpha-\alpha_0)) = 1$ by uniqueness of the function.
	Since $X$ doesn't collapse on hyperplanes by assumption, the result follows from Problem 1.2(b).

\end{enumerate}
\end{problem}

%---------------------------------------------------------------%
%---------------------------------------------------------------%

\section{Stochastic Equicontinuity}
%2.1
\begin{problem}
Let $(\Theta, d)$ be a metric space. Let $(G_n)_{n \in \mathbb{N}}$ and $(Q_n)_{n \in \mathbb{N}}$ be two sequences of random functions, both are stochastically equicontinuous. Then $(G_n + Q_n)_{n \in \mathbb{N}}$ is also s.e.\\

\textbf{Solution:} The result will be a consequence of the triangle inequality.
\begin{center}
$|G_n(\theta)+Q_n(\theta)-G_n(\theta')-Q_n(\theta')|\leq |G_n(\theta)-G_n(\theta')|+|Q_n(\theta)-Q_n(\theta')|$
\end{center}
Then take $\varepsilon>0, \eta>0$. From $G_n, Q_n$ s.e., $\exists \delta_1,\delta_2>0$; $P(\omega(G_n,\delta_1)>\varepsilon)<\eta/2$ and $P(\omega(Q_n,\delta_2)>\varepsilon)<\eta/2$. Define $\delta^*=min\{\delta_1,\delta_2\}$. Hence,
\begin{center}
$\{\omega(G_n+Q_n,\delta^*)>\varepsilon\} \subseteq \{\omega(G_n,\delta^*)>\varepsilon\} \cup \{\omega(Q_n,\delta^*)>\varepsilon\}$
\end{center}
\begin{align*}
&\implies P(\omega(G_n+Q_n,\delta^*)>\varepsilon)\leq P(\omega(G_n,\delta^*)>\varepsilon) + P(\omega(Q_n,\delta^*)>\varepsilon)\\
&\implies \lim_{n\to\infty}\sup P(\omega(G_n+Q_n,\delta^*)>\varepsilon) < \eta/2+\eta/2=\eta
\end{align*}
Since $\varepsilon$ and $\eta$ are arbitrary, we have the result.
\end{problem}

\bigskip
%2.2
\begin{problem} Let $(\Theta, d)$ be a metric space and let $Q$ be a deterministic function on $\Theta$. Then the sequence $(Q_n)_{n \in \mathbf{N}}$ defined by $Q_n = Q$ for every $n \in \mathbb{N}$ is s.e. if and only if $Q$ is uniformly continuous.\\
	
	\textbf{Solution:} \\
	
	If $Q$ is uniformly continuous, then $\forall \epsilon > 0, \exists \delta_\epsilon > 0$ such that $\forall \theta, \theta' \in \Theta, d(\theta, \theta') < \delta_\epsilon \implies \\ |Q(\theta) - Q(\theta')| < \epsilon$.
	
	That is, $\sup\limits_{d(\theta, \theta') < \delta_\epsilon} |Q(\theta) - Q(\theta')| < \epsilon$ and since $Q$ is deterministic $\mathbb{P} (\sup\limits_{d(\theta, \theta') < \delta_\epsilon} |Q(\theta) - Q(\theta')| < \epsilon) = 1$ and $Q_n = Q$ for every $n \in \mathbb{N}$,  $ \limsup\limits_{n \rightarrow \infty} \mathbb{P} (\sup\limits_{d(\theta, \theta') < \delta_\epsilon} |Q_n(\theta) - Q_n(\theta')| > \epsilon) = 0 < \eta$, that is, the sequence $(Q_n)_{n \in \mathbf{N}}$ is s.e..  \\
	
	If  $(Q_n)_{n \in \mathbf{N}}$ is s.e., then    $ \forall \epsilon > 0, \eta > 0, \exists \delta > 0$ such that $\limsup\limits_{n \rightarrow \infty} \mathbb{P} (\sup\limits_{d(\theta, \theta') < \delta} |Q_n(\theta) - Q_n(\theta')| > \epsilon) < \eta$. Since  $Q_n = Q$ for every $n \in \mathbb{N}$, we can drop the $\limsup\limits_{n \rightarrow \infty}$ and get \\ $\mathbb{P} (\sup\limits_{d(\theta, \theta') < \delta} |Q(\theta) - Q(\theta')| > \epsilon) < \eta$. Now since $Q$ is deterministic function and this statement hold $\forall \eta > 0$, we can get that $\sup\limits_{d(\theta, \theta') < \delta} |Q(\theta) - Q(\theta')| < \epsilon$. That is, $Q$ is uniformly continuous.  
\end{problem}

\bigskip
%2.3
\begin{problem}
$(X_t)_{t=1}^{n} \overset{iid}{\sim} N(\mu, v) $. $\Theta = \{ \theta
\coloneqq (\mu,
v): |\mu| \le \overline{\mu}, v \in [\underline{v}, \overline{v}]\}$.
\paragraph*{a} Write the criterion function $Q_n$
\begin{multline*}
  Q_n(\theta) = \tfrac{1}{n}\log f_{\{X_1,...,X_n\}}(x_1,..,x_n;\theta) =
  \tfrac{1}{n}\log \prod_{i=1}^n f_{X_i}(x_i;\theta)  \\ =
  \tfrac{1}{n} \log\left((2\pi v)^{-n/2} \exp
    \left(-\tfrac{\sum(x_i-\mu)^ 2}{2v}\right)\right) =
  -\tfrac{1}{2}\log 2\pi v 
  -\tfrac{1}{2v} \overline{(x_i - \mu)^2}
\end{multline*}

\paragraph*{b} Show $Q_n$ converges uniformly in probability to some
function $Q$. \\ 

First notice $Q_n = \overline{q_i}$ where $q_i =   -\tfrac{1}{2}\log 2\pi v -
  \tfrac{1}{2v} (x_i - \mu)^2$, so that $\{q_i\}_{i=1}^n$ is iid. We
  have 
  \begin{equation*}
    \mathrm{E}q_i =   -\tfrac{1}{2}\log 2\pi v -
  \tfrac{1}{2v} \mathrm{E}\left[(x_i - \mu)^2\right] < \infty
  \end{equation*}
So a LLN gives $Q_n \overset{p}{\rightarrow} \mathrm{E}q_i \equiv Q$.
For uniform convergence,
\begin{align*}
  \sup_{\theta \in \Theta}\left| Q_n(\theta) - Q(\theta)\right| =&
  \sup_{\theta \in \Theta} \left|\tfrac{1}{2v}\left( \overline{(x_i - \mu)^2} -
  \mathrm{E}\left[(x_i - \mu)^2\right] \right) \right| \\
=& \sup_{\theta \in \Theta} \tfrac{1}{2v} \left| \overline{x_i^2} - \mathrm{E} X_i^2 -2
     \mu \left( \overline{x_i} - \mathrm{E} X_i \right) + \cancel{\mu^2}
     - \cancel{\mu^2} \right| \\
\le& \tfrac{1}{2\underline{v}} \left| \overline{x_i^2} -
     \mathrm{E} X_i^2 \right| +
     \frac{\overline{\mu}}{\underline{v}} \left| \overline{x_i} -
     \mathrm{E} X_i \right| = o_p(1)
\end{align*}

\paragraph*{c} Show the MLE estimator is consistent for $\theta_0
\coloneqq \underset{\theta \in \Theta}{\arg\max}Q$ \\

We have $\hat{\theta}_{MLE} = \underset{\theta \in \Theta}{\arg\max}
Q_n(\theta)$ and that $Q_n \xrightarrow[unif]{p} Q$, so it is
sufficient to show $\theta_0$ is the well separated maximum of
$Q$. Since $Q$ is a continuous function on compact set $\Theta$,
$Q$ attains its maximum on $\Theta$. It is therefore sufficient to
show that the maximum is unique. $Q$ being differentiable, the FOC
are:
\begin{equation*}
  \begin{pmatrix}
    \tfrac{1}{v} \mathrm{E}[X_i - \mu] & =0\\ -\tfrac{1}{2v} +
    \tfrac{1}{2v^2}\mathrm{E}\left[(X_i - \mu)^2\right] & =0
  \end{pmatrix}
\end{equation*}
Which are satisfied iff $\mu = \mathrm{E}X_i$ and $v = \mathrm{V}X_i$.
Furthermore, the Hessian matrix of $Q(\theta)$ is
\begin{equation*}
\begin{pmatrix}
  -\tfrac{1}{v} & -\tfrac{1}{2v^2} \mathrm{E}(X_i - \mu) \\ -\tfrac{1}{2v^2} \mathrm{E}(X_i
  - \mu) & \tfrac{1}{2v^2} - \tfrac{1}{v^3} \mathrm{E}\left[(X_i-\mu)^2\right]
\end{pmatrix}
\end{equation*}
Which has determinant
\begin{equation*}
  \frac{1}{v^3}\left( \frac{\mathrm{E}\left[(X_i-\mu)^2\right]}{v} -
  \frac{1}{2} \right)
\end{equation*}
Since this is positive about $\theta_0 = \left(\mathrm{E}X_i,
  \mathrm{V}X_i\right)$, and $-\tfrac{1}{v}<0$, $\theta_0$ will
be the unique maximum on the collection of $(\mu,v)$ for which the
determinant is positive. For uniqueness on $\Theta$, we
should also check $(\mu, v)$ where the Hessian is
not strictly negative definite. (It would be
sufficient for the determinant to be
positive everywhere on $\Theta$, but it is not).


\end{problem}

\bigskip
%2.4
\begin{problem}
\textit{(Median Regression)} Let $Y_t = X_t^T\theta_0 + \varepsilon_t$ where $\theta_0 \in \Theta \subseteq \mathbb{R}^q$ for some $q \in \mathbb{N}$ and compact $\Theta$. Suppose that median$(\varepsilon_t|X_t)=0$. Let $Q_n(\theta) = \frac{1}{n}\sum_{i=1}^n |Y_t - X_t'\theta|$ and $\widehat{\theta_n} = arg\min_{\theta \in \Theta} Q_n(\theta)$.
\paragraph*{a)} Show that $Q_n$ converges in probability uniformly to some deterministic function Q. Design conditions for this to hold. What is your Q function? \\

\textbf{Solution:} We want to apply Theorem 2.b from this lecture notes to get unifromly convergence of $Q_n$. So, we will check its assumptions.
\begin{enumerate}
\item From $\Theta$ compact we can get a totally bounded metric space using $d(\theta,\theta')=||\theta-\theta'||_q = \left(\sum_{i=1}^q |\theta_i-\theta_i'|^q\right)^{1/q}$. 
\item \textit{Assumption 1}: $\{(X_i,Y_i)\}$ are i.i.d.. This allow us to can apply LLN to get our $Q(\theta)=\mathbb{E}(|Y-X'\theta|)$. Hence we define $\tilde{Q}_n(\theta) = Q_n(\theta)-Q(\theta)$. We already know that $\tilde{Q}_n(\theta) = o_p(1)$.
\item Verifying that $\tilde{Q}_n(\theta)$ is s.e. For this we will try to get a Lipschitz-type condition.
\begin{align*}
|\tilde{Q}_n(\theta_1)-\tilde{Q}_n(\theta_2)| &=\frac{1}{n} \left| \sum_{i=1}^n |Y_t - X_t'\theta_1| - |Y_t - X_t'\theta_2|\right|\\
&\leq \frac{1}{n} \left| \sum_{i=1}^n |Y_t - X_t'\theta_2| +|X_t'(\theta_2-\theta_1)| - |Y_t - X_t'\theta_2|\right| \ \text{by Loeve's Cr ineq.}\\
&\leq \frac{1}{n} \sum_{i=1}^n |X_t'(\theta_2-\theta_1)| \leq ||(\theta_2-\theta_1)|| \frac{1}{n} \sum_{i=1}^n ||X_t||
\end{align*}
\textit{Assumption 2}: $\frac{1}{n} \sum_{i=1}^n ||X_t|| = O_p(1)$. This guarantees that we have a Lipschitz-type condition in which $|\tilde{Q}_n(\theta_1)-\tilde{Q}_n(\theta_2)| \leq B_n . h(d(\theta_1,\theta_2))$. This will give s.e. according to Theorem 3.
\end{enumerate}

\paragraph{b)} Show that $\widehat{\theta}_n\overset{P}{\rightarrow}\theta_0$. Be clear about what you assume, and then prove your claim.\\

\textbf{Solution:} To show consistency, we will need to check two assumptions; that $\tilde{Q}_n \overset{p}{\rightarrow}0$ which was showed in the previous item, and that $Q(\theta)$ is uniquely minimized at $\theta=\theta_0$.\\
\textit{Assumption 3}: $\varepsilon_t$ is a continuous r.v. with $f_{\varepsilon_t}(0)>0$. From the lecture of consistency, Q3.b, we know that $f(X)=\theta_0(X)=med(\varepsilon|X)=0$ is the unique minimizer of $Q(\theta)=\mathbb{E}|Y-X'\theta|$.\\
Therefore, we have recovered the assumptions of the consistency theorem for M-estimators and $\hat{\theta}_n\overset{p}{\rightarrow}{\theta_0}=0$.
\end{problem}

\bigskip
%2.5
\begin{problem}
Show that $N (\varepsilon, d) \leq D(\varepsilon, d) \leq N(\varepsilon/2, d)$. \\

\textbf{Solution:}
$N (\varepsilon, d) \leq D(\varepsilon, d)$:
Consider an $\varepsilon$-packing of $\Theta$, $P_{\varepsilon}(\Theta)$, i.e. a collection of $\varepsilon$-separated points, $(p_j \in \Theta)_{j \geq 1}$, such that including any other points would not be $\varepsilon$-separated anymore.
It is obvious that a collection of $\varepsilon$-balls around $(p_j)_{j \geq 1}$ will cover $\Theta$: otherwise, $P_{\varepsilon}(\Theta)$ would not be a packing as another point can be added to $(p_j)_{j \geq 1}$ such that it is still $\varepsilon$-separated.
That is, the packing number is equal to the number of $\varepsilon$-balls that cover $\Theta$.
But then, the minimum number of $\varepsilon$-balls that cover $\Theta$, i.e. the covering number, cannot exceed the packing number: $N (\varepsilon, d) \leq D(\varepsilon, d)$.
\hfill $\blacksquare$

\medskip

$D (\varepsilon, d) \leq N(\varepsilon/2, d)$:
Again, consider an $\varepsilon$-packing of $\Theta$, $P_{\varepsilon}(\Theta)$.
As shown above, $P_{\varepsilon}(\Theta)$ covers $\Theta$.
Now draw $\varepsilon/2$-balls around $(p_j)_{j \geq 1}$.
There are two cases: these balls either cover $\Theta$ or not.
If they do \emph{not} cover $\Theta$, at least one more $\varepsilon/2$-ball should be added to the collection.
If they \emph{do} cover $\Theta$, we argue that this is a minimal $\varepsilon/2$-cover.
Since $P_{\varepsilon}(\Theta)$ is a packing, the minimal distance between any two points is $\varepsilon + \eta$, $\eta \to 0$.
Consequently $\varepsilon/2$-balls around $(p_j)_{j \geq 1}$ will not overlap, they will have at most one common point.
But then, it is not possible to cover $\Theta$ with less $\varepsilon/2$-balls.

To sum up the two cases, the number of $\varepsilon/2$-balls needed to cover $\Theta$ is not less than the $\varepsilon$-packing number.
That is, $D (\varepsilon, d) \leq N(\varepsilon/2, d)$.
\hfill $\blacksquare$

\end{problem}
%---------------------------------------------------------------%
%---------------------------------------------------------------%

\section{Asymptotic Normality}

%3.1
\begin{problem}
$\mathbb{P}(\Omega_n)\rightarrow 1$ \textit{if and only if} $1_{\Omega_n} \overset{P}{\rightarrow}1$.\\

\textbf{Solution:}
\begin{itemize}
\item{($\Longleftarrow$)}: We have that $\forall \varepsilon>0, \lim_{n\to\infty} P(|1_{\Omega_n}-1|>\varepsilon)=0$. In particular, it must hold for $\varepsilon \in (0,1)$. For this case, $P(|1_{\Omega_n}-1|>\varepsilon)=P(\Omega_n^c)$. Therefore, $P(\Omega_n^c) \overset{n \to\infty}{\longrightarrow} 0$ and consequently, $\mathbb{P}(\Omega_n)\rightarrow 1$.

\item{($\implies$)}: We can use Markov inequality to get
\begin{align*}
P(|1_{\Omega_n}-1|>\varepsilon)&\leq \frac{\mathbb{E}((1_{\Omega_n}-1)^2)}{\varepsilon^2}\\
&\leq \frac{P(\Omega_n)+1-2P(\Omega_n)}{\varepsilon^2} \overset{n\to\infty}{\longrightarrow}0
\end{align*}
Then we have the convergence in probability.
\end{itemize}
\end{problem}

\bigskip
%3.2
\begin{problem} (Asymptotic Normality of MLE) Read Theorem 3.3 of Newey-McFadden [NM]. Discuss the role of each assumption. \\

\textbf{Solution:} \\

\textit{Theorem 3.3} Suppose that $z_1, \dots, z_n$ are i.i.d., the hypotheses of Theorem 2.5 are satisfied and 
\begin{enumerate}[(i)]
	\item $\theta_0 \in \text{interior}(\Theta)$
	
	\item $f(z|\theta)$ is twice continuously differentiable and $f(z|\theta) > 0$ in a neighborhood of $\mathcal{N}$ of $\theta_0$
	
	\item $\int \sup_{\theta \in \mathcal{N}} || \nabla_\theta f(z|\theta) || \diff z < \infty, \int \sup_{\theta \in \mathcal{N}} || \nabla_{\theta \theta} f(z|\theta) || \diff z < \infty$
	
	\item $J = \mathbb{E} [\nabla_\theta \ln f(z|\theta_0) \{\nabla_\theta \ln f(z|\theta_0)  \}' ]$ exists and is nonsingular
	
	\item  $ \mathbb{E} [ \sup_{\theta \in \mathcal{N}} || \nabla_{\theta \theta}  \times \ln f(z|\theta)||] < \infty$.
\end{enumerate}
Then $\sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow{d} N(0, J^{-1}).$ \\
 
%Role of assumption: \\
%
%``$z_1, \dots, z_n$ are i.i.d.'': Necessary to use Linberg-Levy CLT and sufficient for LLN. \\
%
%``Hypotheses ($\hat{\theta} \xrightarrow{p}  \theta_0$) of Theorem 2.5 is satisfied'':  it ensures that we can use CMT for $f$ and get w.p.a. 1 result. \\
%
%1.  $\theta_0 \in \text{interior}(\Theta)$: if the distribution is truncated at $\theta_0$, which is on the boundary of $\Theta$, then we could not get asymptotic normality.\footnote{See e.g. example in page 2144 in NM} \\
%
%2. $f(z|\theta)$ is twice continuously differentiable and $f(z|\theta) > 0$ in a neighborhood of $\mathcal{N}$ if $\theta_0$: It ensures that we can characterize $\hat{\theta}$ in terms of the first-order condition, that is, $f(z|\theta)$ is differentiable around $\theta_0$ and it gives that $\nabla_\theta f(z|\theta_0)$ is approaching 0 w.p.a. 1. \\
%
%3. $\int \sup_{\theta \in \mathcal{N}} || \nabla_\theta f(z|\theta) || \diff z < \infty, \int \sup_{\theta \in \mathcal{N}} || \nabla_{\theta \theta} f(z|\theta) || \diff z < \infty$: Both are bounded, therefore we can apply CLT to the first derivative and LLN for the second derivative in the proof.\\
%
%4. $J = \mathbb{E} [\nabla_\theta \ln f(z|\theta_0) \{\nabla_\theta \ln f(z|\theta_0)  \}' ]$ exists and is nonsingular: If $J$ did not exist or were not nonsingular, we could not calculate the asymptotic variance matrix.\\
%
%5. $ \mathbb{E} [ \sup_{\theta \in \mathcal{N}} || \nabla_{\theta \theta}  \times \ln f(z|\theta)||] < \infty$: \\
%
%\textbf{Jackson's solution}
Role of assumptions: \\

Theorem 3.3 is proved by showing the conditions of Theorem 3.1
NM are satisfied. Thm 3.1 is an asymptotic normality result for
M-estimators; similar but slightly weaker than our 
theorem (the difference is that NM 3.1 assumes (primitives for)
uniform convergence of $\nabla_{\theta\theta} Q_n$, whereas we assume
stochastic equicontinuity.) Noticing $f(z|\theta) > 0$ is necessary
for $\log f$ to be defined, each of the other conditions of Theorem 3.3
can be mapped to our asymptotic normality result:

\begin{enumerate}
\item \textbf{A consistent estimator} \\The hypotheses of Th 2.5
  ensure there is a consistent estimator, which is $\arg\min
  \overline{\log f(x_i; \theta)}$

\item $\mathbf{\theta_0 \in \textbf{int}\Theta\textbf{, }Q_n \textbf{
      is }c^2}$ \\
The first part is 3.3(i), the second is from 3.3(ii) 

\item $\mathbf{\nabla_{\theta\theta} Q_n \textbf{ is s.e, }
    \nabla_{\theta\theta} Q_n(\theta_0) - H = o_p(1)}$ \\
These both follow from a ULLN (lemma 2.4). This requires
the data be iid, $\Theta$ compact (constructed in a neighbourhood
about $\theta_0$), $\nabla_{\theta\theta}
Q_n(\theta_0)$ continuous and a dominance condition (3.3(v)). This
uniform convergence result then implies both s.e and p-convergence of $\nabla_{\theta\theta} Q_n(\theta_0)$ to $H =
\mathrm{E}[\nabla_{\theta\theta} \log f(X;\theta_0)]$

\item \textbf{Non-singularity of \textit{H}} \\
3.3(iv) is non-singularity of $J \equiv E[\nabla_{\theta} \log
f(X;\theta_0) \{\nabla_{\theta} \log f(X;\theta_0)\}']$. However,
because the model is correctly specified, 3.3(iii)
and another result (NM lemma 3.6) allow the integral and gradient
operators to be exchanged, so that $J = -H$, so $H$ is also
non-singular. To see $J = -H$, notice $\int f = 1$ implies:
\begin{multline*}
  0 = \nabla_{\theta} \int f =
  \nabla_{\theta\theta'} \int f = \int  \nabla_{\theta \theta'}  f = \int
  \tfrac{1}{f} \nabla_{\theta'} (f \nabla_{\theta} \log f) f = \\
  \mathrm{E} [\tfrac{1}{f} \nabla_{\theta'} (f \nabla_{\theta} \log
  f)] = \mathrm{E}[\nabla_{\theta\theta'}\log f] +
  \mathrm{E}[\nabla_{\theta} \log f \{\nabla_{\theta} \log f\}'] = H + J
\end{multline*}

\item $\mathbf{\sqrt{n}\nabla_{\theta}Q_n(\theta_0) \Rightarrow
    N(0,S)}$ \\
  Here $S= J$ and
  $\sqrt{n}\nabla_{\theta}Q_n(\theta_0) = \sqrt{n}
  \overline{\nabla_{\theta}\log f(x_i; \theta_0)}$. The convergence
  result follows
  easily from a CLT since the data is iid and
  $\nabla_{\theta}\log f(x_i; \theta_0)$ is mean zero with covariance
  matrix $J$.
\end{enumerate}
Applying our theorem, $\sqrt{n}(\hat{\theta}_n - \theta_0) \Rightarrow
-J^{-1} N(0, J) = N(0, J^{-1})$ \\

\end{problem}

\bigskip
%3.3
\begin{problem} (MLE under misspecification) Again, consider Theorem 3.3 of [NM]. Do not assume that ``the hypotheses of [Theorem] 2.5 are satisfied''. Instead, assume that $\hat{\theta}_n \xrightarrow{p} \theta_0$. This is the situation in which we conduct MLE with density function $f(z | \theta)$ without assuming this is the density of the data. As a result, $\theta_0$ is not the ``true parameter'' but only the pseudo-true parameter. Keep other assumptions in the theorem. Derive the asymptotic distribution of $n^{1/2} ( \hat{\theta}_n - \theta_0 )$. \\
	
	\textbf{Solution:} \\
	
In the previous problem, correct specification is only important so
that $J = -H$ in the MLE setting. More precisely, if the true
distribution is $g$ not $f$, then, in the above demonstration of $J =
- H$, the expectation must be taken with respect to $g$, causing the
series of equalities to break down. (Notice, however, since we assume $\theta_0
= \arg\min \mathrm{E}_g[\log f(X;\theta)]$, the regularity conditions
imply $\mathrm{E}_g[\nabla_{\theta}\log f(X;\theta_0)] = 0$.) Indeed,
all conditions for
our M-estimator asymptotic normality theorem still apply, so
\begin{equation*}
  n^{\sfrac{1}{2}}\left( \hat{\theta}_n - \theta_0 \right) \Rightarrow
  N(0, H^{-1}S H^{-1})
\end{equation*}
Where $S = \mathrm{E}_g\left[\nabla_{\theta}\log f(X; \theta_0) \{\nabla_{\theta}\log
f(X; \theta_0)\}'\right]$ and $H = \mathrm{E}_g\left[ \nabla_{\theta\theta'}
  \log f(X; \theta_0)\right]$

\end{problem}

\bigskip
%3.4
\begin{problem}
Under Assumptions 2 and 3 above, $\hat{H}_n \equiv \nabla_{\theta\theta}(\widehat{\theta_n}) \overset{P}{\rightarrow} H$.\\

\textbf{Solution:} We will assume throughout the answer that $\widehat{\theta_n}\overset{p}{\rightarrow}\theta_0$. Also, it will be useful to use the following definition of s.e.; for any two sequences of $\Theta$-valued random variables $\{\theta_{1n}\}_{n\in\mathbb{N}}, \{\theta_{2n}\}_{n\in\mathbb{N}}$ with $d(\theta_{1n},\theta_{2n})\overset{p}{\rightarrow}0$ we have $Q_n(\theta_{1n}) - Q_n(\theta_{2n})\overset{p}{\rightarrow}0$.\\

Assumption 2 and the consistency of $\widehat{\theta_n}$ gives us $\nabla_{\theta\theta}(\widehat{\theta_n}) - \nabla_{\theta\theta}(\widehat{\theta_0}) \overset{p}{\rightarrow}0$ which is equivalent to say $\nabla_{\theta\theta}(\widehat{\theta_n}) \overset{p}{\rightarrow} \nabla_{\theta\theta}(\widehat{\theta_0}) = H(\theta_0)$.\\

Finally, Assumption 3 would allow us to apply CMT and get a consistent estimator for $H^{-1}$.
\end{problem}









\end{document}
