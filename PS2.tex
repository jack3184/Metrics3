\documentclass[11pt,letterpaper]{article}                  % Define document class

%\usepackage{amsfonts} % For \mathbb command
%\usepackage[top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
%\usepackage{amsmath} % for align
\usepackage{mathtools} % for lVert
%\usepackage{graphicx} % for charts

% Set path
\newcommand{\path}{Preamble}

% Text packages
\input{"\path/packText.tex"}
\setlength\parindent{0pt}                                  % No indentation for the whole document

% Figure/table packages
\input{"\path/packFigure.tex"}

% Math packages
\input{"\path/packMath.tex"}

% Graphics packages
\input{"\path/packGraph.tex"}

\allowdisplaybreaks % so that aligned equations can be broken across pages

%% Convenient math abbreviations
\newcommand*\diff{\mathop{}\!d} % nicely formatted integral dx
%\newcommand{\E}{\mathrm{E}} % Expectation operator
\newcommand{\Var}{\mathrm{V}} % Variance operator
\newcommand{\Cov}{\mathrm{Cov}} % Covariance operator
%\newtheorem{problem}{Problem}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert} % For Euclidean norm


% Title
\title{Problem Set 2 \\ \medskip \Large{Econometrics III}}
\author{\Large Jackson Bunting, Attila Gyetvai, Peter Horvath, Leonardo Salim Saker Chaves}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle
\section{Consistency of M-Estimation} 


%1.1
\begin{problem}
\end{problem}

\bigskip
%1.2
\begin{problem}
If $\theta \neq 0$ and $a \in \mathbb{R}$, then
$\mathrm{P}(\theta' X = a) < 1$
\paragraph*{a}
$\theta \in \mathbb{R}^2$ and $a \in \mathbb{R}$. Suppose
$\mathrm{P}(X'\theta = a) = 1$. Therefore $\theta = 0$ by the above
condition. And then $a = 0$.

\paragraph*{b}
$\theta_1,\theta_1 \in \mathbb{R}^2$ and $a_1,a_2 \in
\mathbb{R}$. Suppose $1 = \mathrm{P}(X'\theta_1 + a_1 = X'\theta_2 +
a_2) = \mathrm{P}\left(X'(\theta_2-\theta_1) = (a_1-a_2)\right)$. Part
a gives $\theta_1 = \theta_2$ and for $a$.
\end{problem}

\bigskip
%1.3
\begin{problem}
\begin{enumerate}[(a)]
	\item 
	\begin{align*}
		\E \left[ \E_X \left[ |Y - f(X)| \right] \right] &= \int \int \E_X \left[ |Y - f(X)| \right] f_{Y,X} (y,x) dx \, dy\\
		&= \int \E_X \left[ |Y - f(X)| \right] f_X (x) dx \\
		&= \int \int |y - f(x)| f_{Y | X=x} (y) dy f_X (x) dx \\
		&= \int \int |y - f(x)| f_{Y | X=x} (y) f_X (x) dy \, dx \\
		&= \int \int |y - f(x)| f_{Y, X} (y,x) dy \, dx \\
		&= \E \left[ |Y - f(X)| \right]
	\end{align*}
	The second line follows from the fact that the inner conditional expectation is a function of $x$ only.

	\item Let $f_{Y|X=x} (y) = f_Z (y)$.
	The result follows from Problem 1.1 (b).

	\item Let $f(x) = \alpha + \beta' x$.
	The result follows from Part (b).
\end{enumerate}
\end{problem}

%---------------------------------------------------------------%
%---------------------------------------------------------------%

\section{Stochastic Equicontinuity}
%2.1
\begin{problem}
Let $(\Theta, d)$ be a metric space. Let $(G_n)_{n \in \mathbb{N}}$ and $(Q_n)_{n \in \mathbb{N}}$ be two sequences of random functions, both are stochastically equicontinuous. Then $(G_n + Q_n)_{n \in \mathbb{N}}$ is also s.e.\\

\textbf{Solution:} The result will be a consequence of the triangle inequality.
\begin{center}
$|G_n(\theta)+Q_n(\theta)-G_n(\theta')-Q_n(\theta')|\leq |G_n(\theta)-G_n(\theta')|+|Q_n(\theta)-Q_n(\theta')|$
\end{center}
Then take $\varepsilon>0, \eta>0$. From $G_n, Q_n$ s.e., $\exists \delta_1,\delta_2>0$; $P(\omega(G_n,\delta_1)>\varepsilon)<\eta/2$ and $P(\omega(Q_n,\delta_2)>\varepsilon)<\eta/2$. Define $\delta^*=min\{\delta_1,\delta_2\}$. Hence,
\begin{center}
$\{\omega(G_n+Q_n,\delta^*)>\varepsilon\} \subseteq \{\omega(G_n,\delta^*)>\varepsilon\} \cup \{\omega(Q_n,\delta^*)>\varepsilon\}$
\end{center}
\begin{align*}
&\implies P(\omega(G_n+Q_n,\delta^*)>\varepsilon)\leq P(\omega(G_n,\delta^*)>\varepsilon) + P(\omega(Q_n,\delta^*)>\varepsilon)\\
&\implies \lim_{n\to\infty}\sup P(\omega(G_n+Q_n,\delta^*)>\varepsilon) < \eta/2+\eta/2=\eta
\end{align*}
Since $\varepsilon$ and $\eta$ are arbitrary, we have the result.
\end{problem}

\bigskip
%2.2
\begin{problem}
\end{problem}

\bigskip
%2.3
\begin{problem}
$(X_t)_{t=1}^{n} \overset{iid}{\sim} N(\mu, v) $. $\Omega = \{ \theta
\coloneqq (\mu,
v): |\mu| \le \overline{\mu}, v \in [\underline{v}, \overline{v}]\}$.
\paragraph*{a}
$Q_n(\theta) = \tfrac{1}{2}\log 2\pi + \tfrac{1}{2} \log v +
\tfrac{1}{n2v} \sum(x_i - \mu)$
\end{problem}

\bigskip
%2.4
\begin{problem}
\textit{(Median Regression)} Let $Y_t = X_t^T\theta_0 + \varepsilon_t$ where $\theta_0 \in \Theta \subseteq \mathbb{R}^q$ for some $q \in \mathbb{N}$ and compact $\Theta$. Suppose that median$(\varepsilon_t|X_t)=0$. Let $Q_n(\theta) = \frac{1}{n}\sum_{i=1}^n |Y_t - X_t'\theta|$ and $\widehat{\theta_n} = arg\min_{\theta \in \Theta} Q_n(\theta)$.
\paragraph*{a)} Show that $Q_n$ converges in probability uniformly to some deterministic function Q. Design conditions for this to hold. What is your Q function? \\

\textbf{Solution:} We want to apply Theorem 2.b from this lecture notes to get unifromly convergence of $Q_n$. So, we will check its assumptions.
\begin{enumerate}
\item From $\Theta$ compact we can get a totally bounded metric space using $d(\theta,\theta')=||\theta-\theta'||_q = \left(\sum_{i=1}^q |\theta_i-\theta_i'|^q\right)^{1/q}$. 
\item \textit{Assumption 1}: $\{(X_i,Y_i)\}$ are i.i.d.. This allow us to can apply LLN to get our $Q(\theta)=\mathbb{E}(|Y-X'\theta|)$. Hence we define $\tilde{Q}_n(\theta) = Q_n(\theta)-Q(\theta)$. We already know that $\tilde{Q}_n(\theta) = o_p(1)$.
\item Verifying that $\tilde{Q}_n(\theta)$ is s.e. For this we will try to get a Lipschitz-type condition.
\begin{align*}
|\tilde{Q}_n(\theta_1)-\tilde{Q}_n(\theta_2)| &=\frac{1}{n} \left| \sum_{i=1}^n |Y_t - X_t'\theta_1| - |Y_t - X_t'\theta_2|\right|\\
&\leq \frac{1}{n} \left| \sum_{i=1}^n |Y_t - X_t'\theta_2| +|X_t'(\theta_2-\theta_1)| - |Y_t - X_t'\theta_2|\right| \ \text{by Loeve's Cr ineq.}\\
&\leq \frac{1}{n} \sum_{i=1}^n |X_t'(\theta_2-\theta_1)| \leq ||(\theta_2-\theta_1)|| \frac{1}{n} \sum_{i=1}^n ||X_t||
\end{align*}
\textit{Assumption 2}: $\frac{1}{n} \sum_{i=1}^n ||X_t|| = O_p(1)$. This guarantees that we have a Lipschitz-type condition in which $|\tilde{Q}_n(\theta_1)-\tilde{Q}_n(\theta_2)| \leq B_n . h(d(\theta_1,\theta_2))$. This will give s.e. according to Theorem 3.
\end{enumerate}

\paragraph{b)} Show that $\widehat{\theta}_n\overset{P}{\rightarrow}\theta_0$. Be clear about what you assume, and then prove your claim.\\

\textbf{Solution:} To show consistency, we will need to check two assumptions; that $\tilde{Q}_n \overset{p}{\rightarrow}0$ which was showed in the previous item, and that $Q(\theta)$ is uniquely minimized at $\theta=\theta_0$.\\
\textit{Assumption 3}: $\varepsilon_t$ is a continuous r.v. with $f_{\varepsilon_t}(0)>0$. From the lecture of consistency, Q3.b, we know that $f(X)=\theta_0(X)=med(\varepsilon|X)=0$ is the unique minimizer of $Q(\theta)=\mathbb{E}|Y-X'\theta|$.\\
Therefore, we have recovered the assumptions of the consistency theorem for M-estimators and $\hat{\theta}_n\overset{p}{\rightarrow}{\theta_0}=0$.
\end{problem}

\bigskip
%2.5
\begin{problem}
Show that $N (\varepsilon, d) \leq D(\varepsilon, d) \leq N(\varepsilon/2, d)$. \\

\textbf{Solution:}
$N (\varepsilon, d) \leq D(\varepsilon, d)$:
Consider an $\varepsilon$-packing of $\Theta$, $P_{\varepsilon}(\Theta)$, i.e. a collection of $\varepsilon$-separated points, $(p_j \in \Theta)_{j \geq 1}$, such that including any other points would not be $\varepsilon$-separated anymore.
It is obvious that a collection of $\varepsilon$-balls around $(p_j)_{j \geq 1}$ will cover $\Theta$: otherwise, $P_{\varepsilon}(\Theta)$ would not be a packing as another point can be added to $(p_j)_{j \geq 1}$ such that it is still $\varepsilon$-separated.
That is, the packing number is equal to the number of $\varepsilon$-balls that cover $\Theta$.
But then, the minimum number of $\varepsilon$-balls that cover $\Theta$, i.e. the covering number, cannot exceed the packing number: $N (\varepsilon, d) \leq D(\varepsilon, d)$.
\hfill $\blacksquare$

\medskip

$D (\varepsilon, d) \leq N(\varepsilon/2, d)$:
Again, consider an $\varepsilon$-packing of $\Theta$, $P_{\varepsilon}(\Theta)$.
As shown above, $P_{\varepsilon}(\Theta)$ covers $\Theta$.
Now draw $\varepsilon/2$-balls around $(p_j)_{j \geq 1}$.
There are two cases: these balls either cover $\Theta$ or not.
If they do \emph{not} cover $\Theta$, at least one more $\varepsilon/2$-ball should be added to the collection.
If they \emph{do} cover $\Theta$, we argue that this is a minimal $\varepsilon/2$-cover.
Since $P_{\varepsilon}(\Theta)$ is a packing, the minimal distance between any two points is $\varepsilon + \eta$, $\eta \to 0$.
Consequently $\varepsilon/2$-balls around $(p_j)_{j \geq 1}$ will not overlap, they will have at most one common point.
But then, it is not possible to cover $\Theta$ with less $\varepsilon/2$-balls.

To sum up the two cases, the number of $\varepsilon/2$-balls needed to cover $\Theta$ is not less than the $\varepsilon$-packing number.
That is, $D (\varepsilon, d) \leq N(\varepsilon/2, d)$.
\hfill $\blacksquare$

\end{problem}
%---------------------------------------------------------------%
%---------------------------------------------------------------%

\section{Asymptotic Normality}

%3.1
\begin{problem}
$\mathbb{P}(\Omega_n)\rightarrow 1$ \textit{if and only if} $1_{\Omega_n} \overset{P}{\rightarrow}1$.\\

\textbf{Solution:}
\begin{itemize}
\item{($\Longleftarrow$)}: We have that $\forall \varepsilon>0, \lim_{n\to\infty} P(|1_{\Omega_n}-1|>\varepsilon)=0$. In particular, it must hold for $\varepsilon \in (0,1)$. For this case, $P(|1_{\Omega_n}-1|>\varepsilon)=P(\Omega_n^c)$. Therefore, $P(\Omega_n^c) \overset{n \to\infty}{\longrightarrow} 0$ and consequently, $\mathbb{P}(\Omega_n)\rightarrow 1$.

\item{($\implies$)}: We can use Markov inequality to get
\begin{align*}
P(|1_{\Omega_n}-1|>\varepsilon)&\leq \frac{\mathbb{E}((1_{\Omega_n}-1)^2)}{\varepsilon^2}\\
&\leq \frac{P(\Omega_n)+1-2P(\Omega_n)}{\varepsilon^2} \overset{n\to\infty}{\longrightarrow}0
\end{align*}
Then we have the convergence in probability.
\end{itemize}
\end{problem}

\bigskip
%3.2
\begin{problem}
\end{problem}

\bigskip
%3.3
\begin{problem}
\end{problem}

\bigskip
%3.4
\begin{problem}
Under Assumptions 2 and 3 above, $\hat{H}_n \equiv \nabla_{\theta\theta}(\widehat{\theta_n}) \overset{P}{\rightarrow} H$.\\

\textbf{Solution:} We will assume throughout the answer that $\widehat{\theta_n}\overset{p}{\rightarrow}\theta_0$. Also, it will be useful to use the following definition of s.e.; for any two sequences of $\Theta$-valued random variables $\{\theta_{1n}\}_{n\in\mathbb{N}}, \{\theta_{2n}\}_{n\in\mathbb{N}}$ with $d(\theta_{1n},\theta_{2n})\overset{p}{\rightarrow}0$ we have $Q_n(\theta_{1n}) - Q_n(\theta_{2n})\overset{p}{\rightarrow}0$.\\

Assumption 2 and the consistency of $\widehat{\theta_n}$ gives us $\nabla_{\theta\theta}(\widehat{\theta_n}) - \nabla_{\theta\theta}(\widehat{\theta_0}) \overset{p}{\rightarrow}0$ which is equivalent to say $\nabla_{\theta\theta}(\widehat{\theta_n}) \overset{p}{\rightarrow} \nabla_{\theta\theta}(\widehat{\theta_0}) = H(\theta_0)$.\\

Finally, Assumption 3 would allow us to apply CMT and get a consistent estimator for $H^{-1}$.
\end{problem}









\end{document}
