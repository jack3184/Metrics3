\documentclass[11pt,letterpaper]{article}                  % Define document class

%\usepackage{amsfonts} % For \mathbb command
%\usepackage[top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
%\usepackage{amsmath} % for align
%\usepackage{mathtools} % for lVert
%\usepackage{graphicx} % for charts

% Set path
\newcommand{\path}{Preamble}

% Text packages
\input{"\path/packText.tex"}
\setlength\parindent{0pt}                                  % No indentation for the whole document

% Figure/table packages
\input{"\path/packFigure.tex"}

% Math packages
\input{"\path/packMath.tex"}

% Graphics packages
\input{"\path/packGraph.tex"}

\allowdisplaybreaks % so that aligned equations can be broken across pages

%% Convenient math abbreviations
\newcommand*\diff{\mathop{}\!d} % nicely formatted integral dx
%\newcommand{\E}{\mathrm{E}} % Expectation operator
\newcommand{\Var}{\mathrm{V}} % Variance operator
\newcommand{\Cov}{\mathrm{Cov}} % Covariance operator
%\newtheorem{problem}{Problem}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert} % For Euclidean norm


% Title
\title{Problem Set 3 \\ \medskip \Large{Econometrics III}}
\author{\Large Jackson Bunting, Attila Gyetvai, Peter Horvath, Leonardo Salim Saker Chaves}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle
\section{Moment Estimation}

%1.1
\begin{problem} Consider an alternative proof for the asymptotics of the GMM estimator by applying the proof strategy for general M-estimators discussed in the ``Asymptotic Normality" notes. \\

\textbf{Solution:} $\hat{\theta}_n = \arg\min \bar{g}_n(\theta)^\intercal W_n \bar{g}_n(\theta) = \arg\min Q_n(\theta)$

Therefore, $\nabla_\theta Q_n(\hat{\theta}_n) = 0$. In order to take the Taylor expansion around $\theta_0$, we need to assume that $\bar{g}_n(\theta)$ is twice continuously differentiable in a neighborhood of $\theta_0$. 

So we can get:

\begin{equation*}
	\nabla_\theta Q_n(\hat{\theta}_n) = 0 = \nabla_\theta Q_n(\theta_0) +  \nabla_{\theta \theta} Q_n(\bar{\theta}) (\hat{\theta}_n - \theta_0),
\end{equation*} where

\begin{equation*}
\nabla_\theta Q_n(\theta_0) = \bar{g}_n(\theta_0)^\intercal W_n \bar{g}_n(\theta_0) = G^\intercal W \bar{g}_n(\theta_0) + o_p(1)
\end{equation*}

%\begin{equation*}
\begin{align*}
\nabla_{\theta \theta} Q_n(\bar{\theta})  & = \nabla_{\theta \theta} \bar{g}_n(\bar{\theta})^\intercal W_n\bar{g}_n(\bar{\theta}) +  \nabla_{\theta} 
\bar{g}_n(\bar{\theta})^\intercal W_n \bar{g}_n(\bar{\theta}) \\
& = \nabla_{\theta \theta} \bar{g}_n(\bar{\theta})^\intercal W_n\bar{g}_n(\theta_0) + G^\intercal W G + o_p(1) \\ 
& =O_p(1)o_p(1)+G^\intercal W G + o_p(1) \\
& = G'WG+o_p(1). 
\end{align*}
%\end{equation*}

Therefore, we can get that 

\begin{align*}
	0 = G' W \bar{g}_n(\bar{\theta}_0) + G' W G (\hat{\theta}_n - \theta_0)+o_p(1),
\end{align*}
which implies 

\begin{align*}
	\sqrt{n}(\hat{\theta}_n - \theta_0) = (G^\intercal W G)^{-1} G^\intercal W \sqrt{n} \bar{g}_n(\theta_0) \xrightarrow{d} N(0; (G^\intercal W G)^{-1} G^\intercal W S W G (G^\intercal W G)^{-1}).
\end{align*}

\end{problem}

\bigskip
%1.2
\begin{problem} Write down high-level conditions in the style of the lecture notes (or Newey-McFadden's handbook chapter). under these assumptions, derive the consistency and symptotic normality of the CUE. Verify that it attains the assymptotic variance using the optimal weighting matrix. \\
	
\textbf{Solution:} 
\begin{align*}
	\hat{\theta}_n = \arg\min\limits_{\theta \in \Theta} \bar{g}_n(\theta)^\intercal \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta) = \arg\min_{\theta \in \Theta} Q_n(\theta)
\end{align*}

Obs: $\partial A^{-1} = - A^{-1} \partial A A^{-1}$, $\partial (A \cdot B) = \partial A \cdot B + A \cdot B$ and $vec(d U) = d vec(U)$. 
	
FOC: 

\begin{align*}
	\nabla_{\theta} Q_n(\bar{\theta}_n)=0,
\end{align*} 

that is,
\begin{align*}
\underbrace{\nabla_{\theta} Q_n(\bar{\theta}_n)}_{d \times 1}= \nabla_\theta \bar{g}_n(\theta)^\intercal \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta) + \left(\frac{\partial}{\partial \theta} \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta)    \right)^\intercal \bar{g}_n(\theta),
\end{align*} 

Obs:

\begin{align*}
	\frac{\partial}{\partial \theta} \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta) & = \frac{\partial}{\partial \theta} \left(\bar{g}_n(\theta) \otimes I_n  \right) \cdot vec(\hat{S}_n(\theta)^{-1}) \\
	& = \underbrace{\hat{S}_n(\theta)^{-1} \nabla \bar{g}_n(\theta)}_{k \times d} - \underbrace{\left(\bar{g}_n(\theta)^\intercal \otimes I_k \right) \left(\hat{S}_n(\theta)^{-1} \otimes \hat{S}_n(\theta)^{-1} \right) \frac{\partial vec(\hat{S}_n(\theta))}{\partial}}_{k \times d},
\end{align*}

which implies

\begin{align*}
	0 & = \nabla_\theta \bar{g}_n(\theta)^\intercal \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta)  + \nabla_\theta \bar{g}_n(\theta)^\intercal \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta)  - \underbrace{\frac{\partial vec(\hat{S}_n(\theta))}{\partial \theta}^\intercal \cdot \left(\hat{S}_n(\theta)^{-1} \bar{g}_n(\hat{\theta}_n) \otimes  \hat{S}_n(\theta)^{-1} \bar{g}_n(\hat{\theta}_n) \right)}_{d \times k^2} \\
	& = 2 \nabla_\theta \bar{g}_n(\theta)^\intercal \hat{S}_n(\theta)^{-1} \bar{g}_n(\theta) - \frac{\partial vec(\hat{S}_n(\theta))}{\partial \theta}^\intercal \cdot \left(\hat{S}_n(\theta)^{-1} \bar{g}_n(\hat{\theta}_n) \otimes  \hat{S}_n(\theta)^{-1} \bar{g}_n(\hat{\theta}_n) \right)
\end{align*}

Obs: 
\begin{align*}
	\frac{\partial vec(\hat{S}_n(\theta))}{\partial \theta}^\intercal \cdot \left(\hat{S}_n(\theta)^{-1} \bar{g}_n(\hat{\theta}_n) \otimes  \hat{S}_n(\theta)^{-1} \bar{g}_n(\hat{\theta}_n)  \right) = o_p(1)
\end{align*}

If we assume $S(\cdot)$ continuous and $\sup\limits_{\theta \in \mathcal{N}} || \hat{S}_n(\theta) - S_(\theta) || \xrightarrow{p} 0$, we can conclude the above result. 

Hence, we will have something similar to the GMM proof in the notes

\begin{align*}
	0  = \nabla_\theta \bar{g}_n(\hat{\theta}_n)^\intercal \hat{S}_n(\hat{\theta}_n)^{-1} \bar{g}_n(\hat{\theta}_n) + o_p(1).
\end{align*}

For MVT,

\begin{align*}
 0 &=  	\nabla_\theta \bar{g}_n(\hat{\theta}_n)^\intercal \left(\bar{g}_n(\theta_0)+ \nabla_\theta \bar{g}_n(\bar{\theta}_n) (\hat{\theta}_n - \theta_0) \right) + o_p(1) \\
 & = (G^\intercal S^{-1} + o_p(1))\left(\bar{g}_n(\theta_0)+ G (\hat{\theta}_n - \theta_0) + o_p(1) \right) + o_p(1), 
\end{align*}

which implies
\begin{align*}
	\sqrt{n}(\hat{\theta}_n - \theta_0) = (G^\intercal S^{-1} G)^{-1} G^\intercal S^{-1} \sqrt{n} \hat{g}_n(\theta)_0 + o_p(1) \xrightarrow{d} (G^\intercal S^{-1} G)^{-1} G^\intercal S^{-1} N(0, S) \sim N(0, (G^\intercal S^{-1} G)^{-1}).
\end{align*}

\end{problem}

\bigskip
%1.3
\begin{problem}
Suppose that $\hat{\eta}_n$ is an M-estimator obtained by maximizing $\tilde{Q}_n (\eta)$.
Suppose that 
\begin{align*}
	\begin{pmatrix} \sqrt{n} \bar{g}_n (\theta_0, \eta_0) \\ \sqrt{n} \nabla_n \tilde{Q}_n (\eta_0) \end{pmatrix} \overset{d}{\to}
	N \left( 0, \begin{pmatrix} S_{gg} & S_{g \eta} \\ S_{\eta g} & S_{\eta \eta} \end{pmatrix} \right).
\end{align*}
Following the discussion above, derive the asymptotic distribution of $\sqrt{n} (\hat{\theta}_n - \theta_0)$.

\medskip
\textbf{Solution:} From before,
\begin{align*}
	\sqrt{n} \bar{g}_n (\theta_0, \hat{\eta}_n) &= \sqrt{n} \bar{g}_n (\theta_0, \eta_0) + (\Gamma + o_p(1)) \sqrt{n} (\hat{\eta}_n - \eta_0).
	\intertext{Assume that the conditions for the asymptotic normality of $\sqrt{n} (\hat{\eta}_n - \eta_0)$ hold; specifically, $\nabla_{\eta \eta} \bar{Q}_n (\eta_0) \overset{p}{\to} H(\eta_0)$. Then}
	\sqrt{n} (\hat{\eta}_n - \eta_0) &= - (H(\eta_0)^{-1} + o_p(1)) \sqrt{n} \nabla_{\eta} \bar{Q} (\eta_0) \\
	&= H(\eta_0)^{-1} \sqrt{n} \nabla_{\eta} \bar{Q} (\eta_0) + o_p(1) O_p(1).
	\intertext{Using CMT, it follows that}
	\sqrt{n} \bar{g}_n (\theta_0, \hat{\eta}_n) &= \begin{pmatrix} I_k, & \Gamma H(\eta_0)^{-1} \end{pmatrix}
	\begin{pmatrix} \sqrt{n} \bar{g}_n (\theta_0, \eta_0) \\ \sqrt{n} \nabla_{\eta} \bar{Q}_n (\eta_0) \end{pmatrix}
	+ o_p(1).
	\intertext{Consequently}
	\sqrt{n} \bar{g}_n (\theta_0, \hat{\eta}_n) &\overset{d}{\to} \begin{pmatrix} I_k, & \Gamma H(\eta_0)^{-1} \end{pmatrix} N \left( 0, \begin{pmatrix} S_{gg} & S_{g \eta} \\ S_{\eta g} & S_{\eta \eta} \end{pmatrix} \right) \\
	&\sim N \Bigg( 0, \underbrace{\begin{pmatrix} S_{gg} + \Gamma H(\eta_0)^{-1} S_{g \eta}, & S_{\eta g} + \Gamma H(\eta_0)^{-1} S_{\eta \eta} \end{pmatrix} \begin{pmatrix} I_k \\ H(\eta_0)^{-1} \Gamma^{\intercal} \end{pmatrix}}_{=: A} \Bigg).
	\intertext{Using that $S_{\eta \eta} = H(\eta_0)^{-1}$, $A$ can be written as}
	A &= S_{gg} + \Gamma H(\eta_0)^{-1} S_{g \eta} + S_{\eta g} H(\eta_0)^{-1} \Gamma^{\intercal} + \Gamma H(\eta_0)^{-3} \Gamma^{\intercal}.
	\intertext{Then, using that $\sqrt{n} (\hat{\theta}_n -\theta_0) = - ((G^{\intercal} W G)^{-1} G^{\intercal} W + o_p(1)) \sqrt{n} \bar{g}_n (\theta_0, \hat{\eta}_n)$,}
	\sqrt{n} (\hat{\theta}_n - \theta_0) &\overset{d}{\to} (G^{\intercal} W G)^{-1} G^{\intercal} W N(0,A).
	\intertext{In the special case when the first stage estimation is independent from the estimation of $\theta$, we have that $S_{g \eta} = S_{\eta g} = 0$.
	Using the optimal weighting matrix $W = S_{gg}^{-1}$, the variance matrix modifies to}
	\mathrm{Avar} (\hat{\theta}_n) &= (G^\intercal W G)^{-1} + (G^\intercal W G)^{-1} G^\intercal W \Gamma H(\eta_0)^{-3} \Gamma^{\intercal} W G (G^{\intercal} W G)^{-1}.
\end{align*}

\end{problem}

\bigskip
%1.4
\begin{problem}
Verify that $f \mapsto \sup_{\tau \in \mathcal{T}} \| f(\tau) \| $ is continuous
under $\|f\|_{\infty}$

That is, we want to show
\begin{equation*}
\forall \epsilon, \exists \delta : \|f-g\|_{\infty} < \delta
\Rightarrow  \bigl|\sup\|f\|-\sup\|g\| \bigr| < \epsilon
\end{equation*}
Given $\epsilon$, $\delta=\epsilon$ is sufficient since
\begin{equation*}
  \bigl|\sup\|f\|-\sup\|g\| \bigr| \le \bigl|\sup\{\|f\|-\|g\|\}
  \bigr| \le \sup\|f-g\| < \epsilon
\end{equation*}
\end{problem}

\bigskip
%1.5
\begin{problem}
Consider the case when $\theta_0$ is unknown.
Suppose that $\hat{\theta}_n$ is an estimator with the following asymptotic linear representation:
$\sqrt{n} \left( \hat{\theta}_n - \theta_0 \right) = J^{-1} \Delta_n + o_p(1)$ where $J$ is a constant nonsingular matrix and
\begin{align*}
	\Delta_n = \frac{1}{\sqrt{n}} \, \sum_i \gamma (W_t, \theta_0).
\end{align*}
Let $\bar{g}_n (\tau) = \frac{1}{n} \sum_{t=1}^n g \left( W_t, \hat{\theta}_n, \tau \right)$.
Solve the following:
\begin{enumerate}[(a)]
	\item for each $\tau$, derive the asymptotic distribution of $(\sqrt{n} \bar{g}_n (\tau), \sqrt{n} \bar{g}_n (\tau'))$;
	\item if $\sqrt{n} \bar{g}_n (\cdot) \Rightarrow v(\cdot)$, characterize the asymptotic distribution of the process $v(\cdot)$.
\end{enumerate}

\medskip
\begin{enumerate}[(a)]
\item Let's revisit the GMM assumptions.

\begin{enumerate}[(1)]
	\item $g(\cdot)$ is continuously differentiable in a neighborhood of $\theta_0$.
	\item $\theta_0 \in {\rm int} \, \Theta$.

	These two assumptions guarantee that a Taylor approximation exists:
	\begin{align*}
		\bar{g}_n (\tau) &= \bar{g}_n (\theta_0, \tau) + \nabla_{\theta} \bar{g}_n (\bar{\theta}, \tau) (\hat{\theta}_n - \theta_0).
	\end{align*}

	\item For all $\tau \in T$, $\bar{g}_n (\theta_0, \tau) \pto \E (g(W, \theta_0, \tau)) = 0$.
	\item $\nabla_{\theta} \bar{g}_n$ is s.e. in $\theta$ in a neighborhood of $\Theta$ and $\nabla_{\theta} \bar{g}_n (\theta_0, \tau) \pto G(\tau)$.
	\item $\sqrt{n} \left( \hat{\theta}_n - \theta_0 \right) = J^{-1} \Delta_n + o_p(1)$.
\end{enumerate}

Then
\begin{align*}
	\sqrt{n} \bar{g}_n (\tau) &= G(\tau) J^{-1} \Delta_n + o_p (1)
	\intertext{and hence}
	G(\tau) J^{-1} N (0, \Gamma) &\sim N \left( 0, G(\tau) J^{-1} \Gamma J^{-1} G(\tau) \right).
\end{align*}

Invoke the Cramer-Wold device to infer the asymptotic distribution of $(\sqrt{n} \bar{g}_n (\tau), \sqrt{n} \bar{g}_n (\tau'))$.
Let $\lambda \in \mathbb{R}^{2k}$.

\begin{align*}
	\lambda' \begin{pmatrix} \sqrt{n} \bar{g}_n (\tau) \\ \sqrt{n} \bar{g}_n (\tau') \end{pmatrix} &= \frac{1}{\sqrt{n}} \, \sum_{t=1}^n \lambda_1' g \left( W_t, \hat{\theta}_n, \tau \right) + \lambda_2' g \left( W_t, \hat{\theta}_n, \tau' \right) \\
	&= \frac{1}{\sqrt{n}} \, \sum_{t=1}^n \lambda_1' g \left( W_t, {\theta}_0, \tau \right) + \lambda_2' g \left( W_t, {\theta}_0, \tau' \right) \\
	& \qquad + \left( \lambda_1' \nabla_{\theta} g \left( W_t, \bar{\theta}, \tau \right)  + \lambda_2' \nabla_{\theta} g \left( W_t, \bar{\theta}, \tau' \right) \right) (\hat{\theta}_n - \theta_0) \\
	&\dto \left( \lambda_1' G(\tau) + \lambda_2' G(\tau) \right) N ( 0, \underbrace{J^{-1} \Gamma J^{-1}}_{=: S} ) \\
	&= N \left( 0, \lambda_1' G(\tau) S G(\tau)' \lambda_1 + \lambda_1' G(\tau) S G(\tau')' \lambda_2 + \lambda_2' G(\tau') S G(\tau)' \lambda_1  + \lambda_2' G(\tau') S G(\tau')' \lambda_2 \right) \\
	&= \lambda' N \left( 0, \begin{pmatrix} G(\tau) S G(\tau)' & G(\tau) S G(\tau')' \\ G(\tau') S G(\tau)' & G(\tau') S G(\tau')' \end{pmatrix}  \right)
\end{align*}

\item It follows from part (a) that $v(\cdot) \sim N \left( 0, G(\cdot) J^{-1} \Gamma J^{-1} G(\cdot)' \right)$.
The covariance of the process is given by
\begin{align*}
	\Cov (v(\tau), v(\tau')) = G(\tau) S G(\tau')'.
\end{align*} 
\end{enumerate}

\end{problem}

\bigskip
%1.6
\begin{problem}
Suppose $Q_n (\theta, \eta)$ is s.e. in $(\theta, \eta)$.
Show that $Q_n (\theta, \eta)$ is s.e. in $\eta$ for each fixed $\theta$.

\medskip
\begin{proof}
By definition, for all $\varepsilon > 0$, $\eta > 0$ exists $\delta > 0$ such that
\begin{align*}
	\limsup_{n \to \infty} \, & {\rm P} (\omega(Q_n, \delta) > \varepsilon) < \eta
	\intertext{where}
	\omega(Q_n, \delta) &= \sup_{\substack{\norm{(\theta, \eta) - (\theta', \eta')} < \delta \\ (\theta, \eta) \in \Theta \\ (\theta', \eta') \in \Theta }} \abs{Q_n (\theta, \eta) - Q_n (\theta', \eta')}.
	\intertext{Fixing $\theta$ yields}
	\omega(Q_n (\eta), \delta) &= \sup_{\substack{\norm{(\theta, \eta) - (\theta, \eta')} < \delta \\ (\theta, \eta) \in \Theta }} \abs{Q_n (\theta, \eta) - Q_n (\theta, \eta')} \leq \omega(Q_n, \delta).
	\intertext{Therefore for all $n \in N$,}
	& {\rm P} (\omega(Q_n (\eta), \delta) > \varepsilon) \leq {\rm P} (\omega(Q_n, \delta) > \varepsilon);
	\intertext{hence}
	\limsup_{n \to \infty} \, & {\rm P} (\omega(Q_n (\eta), \delta) > \varepsilon) \leq \limsup_{n \to \infty} \, {\rm P} (\omega(Q_n, \delta) > \varepsilon) < \eta.
\end{align*}
That is, $Q_n (\theta, \eta)$ is s.e. for a fixed $\theta$.
\end{proof}

\end{problem}

\bigskip
%1.7
\begin{problem}
Suppose that $\bar{g}_n (\theta, \tau) = \frac{1}{n} \sum_{t=1}^n (\theta X_t + \tau(X_t))$ and $\sup_t \E\abs{X_t} < \infty$.
Equip $\mathcal{T}$ with the uniform metric.
Show that $\bar{g}_n (\cdot)$ is s.e.

\medskip
\begin{proof}
Using the sup norm on $\mathcal{T}$, define the metric $d((\theta, \tau))$ for $\Theta \times \mathcal{T}$ such that 
\begin{align*}
	d((\theta, \tau)) = \abs{\theta} + \sup_{x \in \mathbb{R}} \abs{\tau(x)}.
\end{align*}
Furthermore, from the triangle inequality, we have
\begin{align*}
	\abs{\bar{g}_n (\theta, \tau) - \bar{g}_n (\theta', \tau')} \leq \frac{1}{n} \sum_{t=1}^n \abs{\theta X_t - \theta' X_t} + \abs{\tau(X_t) - \tau'(X_t)}.
\end{align*}
Therefore
\begin{align*}
	\omega (\bar{g}_n, \delta) \leq \frac{1}{n} \, \delta \sum_{t=1}^n \abs{X_t} + \frac{1}{n} \, \delta n \leq \delta + \delta \, \frac{1}{n} \sum_{t=1}^n \abs{X_t}.
\end{align*}
Notice that 
\begin{align*}
	\E \abs{\frac{1}{n} \sum_{t=1}^n \abs{X_t}} &= \E \left( \frac{1}{n} \sum_{t=1}^n \abs{X_t} \right) = \frac{1}{n} \sum_{t=1}^n \E \abs{X_t} \leq \frac{1}{n} \, nK = K
\end{align*}
for some $K$ by assumption.
Hence $\frac{1}{n} \, \sum_{t=1}^n \abs{X_t} = O_p (1)$; that is, for all $\gamma > 0$ exists $M \in \mathbb{R}$ such that $\rm{P} \left( \frac{1}{n} \, \sum_{t=1}^n \abs{X_t} < M \right) > 1 - \gamma$.
Consequently
\begin{align*}
	{\rm P} (\omega(\bar{g}_n, \delta) > \varepsilon) \leq {\rm P} \left( \frac{1}{n} \, \sum_{t=1}^n \abs{X_t} > \frac{\varepsilon - \delta}{\delta} \right).
\end{align*}
Now using the definition of boundedness in probability, for all $\eta > 0$ exists $M(\eta) \in \mathbb{R}$ such that ${\rm P} \left( \frac{1}{n} \, \sum_{t=1}^n \abs{X_t} > M(\eta) \right)$.
Set $M(\eta) = \frac{\varepsilon-\delta}{\delta} \iff \delta = \frac{\varepsilon}{M(\eta) + 1}$ where $\delta > 0$ since $\varepsilon, M(\eta) > 0$.

As a consequence, for all $\varepsilon, \eta > 0$ exists $\delta_{\varepsilon, \eta} > 0$ such that
\begin{align*}
	\lim \sup_{n \to \infty} {\rm P} (\omega (\bar{g}_n, \delta_{\varepsilon, \eta}) > \varepsilon) \leq \lim \sup_{n \to \infty} {\rm P} \left( \frac{1}{n} \, \sum_{t=1}^n \abs{X_t} > \frac{\varepsilon-\delta_{\varepsilon, \eta}}{\delta_{\varepsilon, \eta}} \right) < \eta.
\end{align*}
That is, $\bar{g}_n$ is s.e.
\end{proof}

\end{problem}

\section{Empirical Process}

%2.1
\begin{problem} Show that the outer probability $\mathbb{P}^*$ is subadditive. That is, for $A \subseteq \Omega$ and $B \subseteq \Omega, \mathbb{P}^*(A \cup B) \leq \mathbb{P}^*(A) + \mathbb{P}^*(B)$. (Hint: for C = A or B, approximate $\mathbb{P}^*(C)$ with $\mathbb{P}(C')$ for some measurable C'.) \\
	
	\textbf{Solution:}  Let $A \cup B \subseteq C$ for some C. We know that $\mathbb{P}^*(C) = \inf \{\mathbb{P}(C'): C \subseteq C', \, C'$ is measurable$\}$. 
	
	$\mathbb{P}^*(A) = \inf \{\mathbb{P}(A'): A \subseteq A', \, A'$ is measurable$\}$ and $\mathbb{P}^*(B) = \inf \{\mathbb{P}(B'): B \subseteq B', \, B'$ is measurable$\}$, and $A \cup B \subseteq A' \cup B' \subseteq C'$. 
	
	$\mathbb{P}^*(A \cup B) = \inf \{\mathbb{P}(A' \cup B'): A \cup B \subseteq A' \cup B', \, A'$ and $B'$ are measurable$\} \leq \inf \{\mathbb{P}(A') +  \mathbb{P}(B'): A \subseteq A'$ and $B \subseteq B', \, A'$ and $B'$ are measurable$\}$, since $\mathbb{P}$ is subadditive. And $\inf \{\mathbb{P}(A') +  \mathbb{P}(B'): A \subseteq A'$ and $B \subseteq B', \, A'$ and $B'$ are measurable$\}$ = $\inf \{\mathbb{P}(A'): A \subseteq A', \, A'$ is measurable$\}$ + $\inf \{\mathbb{P}(B'): B \subseteq B', \, B'$ is measurable$\}$, that is, we get $\mathbb{P}^*(A \cup B) \leq \mathbb{P}^*(A) + \mathbb{P}^*(B)$.
\end{problem}

\bigskip
%2.2
\begin{problem}
Show that the below condition is equivalent $\forall a \in (0,\infty]$
\begin{equation*}
  \int_0^a \sup \sqrt{\smash[b]{  \log \underbrace{N\left(\epsilon
\|F\|_2,{Q},\mathcal{F})\right) }_{N_Q(\epsilon)}} }d\epsilon < \infty
\end{equation*}
\begin{equation*}
 N_Q(\epsilon) =\inf\left\{m:\exists (f_i)^m_{i=1} \subset \mathcal{F}\text{ st }
  \sup_f\left\{\min_{1\le j \le m}\|f-f_j\|_{Q,2}\right\} < \epsilon \|F\| \right\}
\end{equation*}
$\Rightarrow (\forall Q \in \mathcal{Q}) $, $N_Q(\epsilon)$ is
decreasing in $\epsilon$. Therefore $\log N_Q(\epsilon)$ is decreasing
in $\epsilon$. Therefore $\sup_{Q}\sqrt{\log N_Q(\epsilon)}$ is also.
Therefore $  \int_0^1 \sup \sqrt{  \log {N_Q(\epsilon)} } d\epsilon
< \infty$ iff $(\forall a \in (0,\infty)$)
\begin{equation*}
  \int_0^a \sup \sqrt{  \log {N_Q(\epsilon)} } d\epsilon < \infty
\end{equation*} 

Now consider $\epsilon \rightarrow \infty$. We have
$\sup_\tau|f(W,\tau)| \le F(W)$, so for any collection $(f_i)^m
\subset \mathcal{F}$ and $Q$, 
\begin{align*}
   \sup_f\left\{\min_{1\le j \le m}\|f-f_j\|_{Q,2}\right\}\le \|2F\|_{Q,2} < \infty
\end{align*}
And $\|2F\|_{Q,2} < \epsilon \|F \|$ as  $\epsilon \rightarrow
\infty$. Therefore for all $\epsilon$ large enough, $\forall (f_i)^m
\subset \mathcal{F}$ and $Q$,
\begin{align*}
   \sup_f\left\{\min_{1\le j \le m}\|f-f_j\|_{Q,2}\right\} \le
  \epsilon \|F\|
\end{align*}
And $N(\epsilon,Q)=1$. So as $\epsilon \rightarrow \infty$,
\begin{equation*}
   \sup \sqrt{  \log {N_Q(\epsilon)} } = 0
\end{equation*} 
So the tail of $\epsilon \uparrow \infty$ is inconsequential to PEC.

\end{problem}

\bigskip
\section{Entropy Conditions}

%3.1
\begin{problem} Consider $\mathcal{C}\equiv \{(-\infty, a_1] \times (-\infty, a_2]; a_1,a_2 \in \mathbb{R}\}$. Show that the VC-index of $\mathcal{C}$ is not greater than 3.\\

\textbf{Solution:} We are considering $\chi = \mathbb{R}^2$. (BWOC) Suppose that VC-index is greater than 3, let's say 4. Then, $\mathcal{C}$ can shatter any $\chi_0 \subseteq \mathbb{R}^2$ with 4 elements. Take $X_0 = \{(1,0); (1,-1), (-1,1)\}$\\

There isn't $C \in \mathcal{C}$; $\{(1,0)\} = C \cap \chi_0$ because you will need to include $\{(1,-1)\}$ on it.\\

$\implies$ VC-index must be less than 4.
\end{problem}

\bigskip
%3.2
\begin{problem} Take this lemma (Pollard 1984, lemma 18) as given. Show that $\mathcal{C}\equiv \{\{g>0\}; g\in \mathcal{G}\}$ forms a VC-class.\\

\textbf{Solution:} Using similar steps as the lemma, we get that $\mathcal{C}' = \{\{s\in S; g(s)>0\}; g \in \mathcal{G}\}$ forms a VC-class. Then by stability properties of VC-class, $\mathcal{C}'^{c}$ will also fomr a VC-class. But notice that $\mathcal{C}'^{c} = \mathcal{C}$. So, we have the proof. 
\end{problem}

\bigskip
%3.3
\begin{problem} Use this lemma (Pollard 1984, lemma 18) to show that the collection of all closed half spaces in $\mathbb{R}^2$ (consisting of points below or above some line in$\mathbb{R}^2$) forms a VC-class.\\

\textbf{Solution:} Any line in $\mathbb{R}^2$ can be written as $0=ax-y+b$, for $a,b \in \mathbb{R}$. The points above the line will be characterized by $\{(x,y)\in \mathbb{R}^2; 0\leq ax-y+b\}$. Therefore we can write the collection of all closed half spaces as:
\begin{center}
$\mathcal{C}=\{\{(x,y)\in \mathbb{R}^2; g(x,y)\geq 0\}; g \in \mathcal{G}\}$ where \\
$\mathcal{G}=\{g:\mathbb{R}^2\mapsto \mathbb{R}; g= y-ax-b = f_1(x,y) - a f_2(x,y) - bf_3(x,y) \text{and } a,b \in \mathbb{R}\}$
\end{center}
Therefore, according to Pollard's lemma, $\mathcal{C}$ will form a VC-class with index less than 3. For the points below, $\mathcal{C}_1=\{\{(x,y)\in \mathbb{R}^2; g(x,y)\leq 0\}; g \in \mathcal{G}\}$ we can use a similar argument to show that they will form a VC-class. Finally, we can characterize the collection of all closed half spaces in $\mathbb{R}^2$ as the union of two sets, $\mathcal{C} \cup \mathcal{C}_1$. Since, each of these sets forms a VC-class, by stability property of VC-class, we have the desired result. 
\end{problem}

\bigskip
%3.4
\begin{problem} Show that $\mathcal{C}\equiv \{\text{star-shaped subsets of } \mathbb{R}^2 \ \text{that looks like } "\star" \}$ forms a VC-class.\\

\textbf{Solution:} Notice that star-shaped subsets in $\mathbb{R}^2$ can be made form the intersection of points below and above a finite number of lines. Hence, by stability of VC-class of sets, the collection of star shaped subsets in $\mathbb{R}^2$ will also form a VC-class.
\end{problem}

\bigskip
%3.5
\begin{problem} Suppose that $\mathcal{F}$ is a VC-major class and $\sup_{w \in \mathcal{W}} |f(x)| \leq K$ for some constant K and all $f \in \mathcal{G}$. Show that $\mathcal{F}$ satisfies Pollard's entropy condition.\\
\textit{(Hint: use stability properties and the above result.)}\\

\textbf{Solution:} Initially, notice that $0\leq \sup_{w \in \mathcal{W}} |f(x)/k|\leq 1$ by assumption. Based on this, we can construct functions $\tilde{f}:W\mapsto [0,1]; \tilde{f}(w) = f(w)/k$ where $f(\cdot) \in \mathcal{F}$.\\
From previous result, the class of functions $\tilde{\mathcal{F}}$ will be a VC-major class. Now, notice that any $\tilde{f}(\cdot)$ is the limit of a sequence:
\begin{center}
$\tilde{f}_m = \sum_{i=1}^m 1/m 1[\tilde{f}>i/m]$
\end{center}
Consequently, $\tilde{\mathcal{F}}$ will be the closure of the convex hull of the following:
\begin{center}
$\{1[\tilde{f}>i/m]; 1\leq i \leq m, m\geq 1, \tilde{f} \in \tilde{\mathcal{F}}\}$
\end{center}
which will form a VC-class. Hence, we can apply Theorem 2 form this lecture notes to this class of indicator functions followed by Theorem 4 also from this lecture notes in order to have $\tilde{\mathcal{F}}$ satisfying Pollard entropy condition.\\

Given that the transformation from $\mathcal{F}$ to $\tilde{\mathcal{F}}$ is just the multiplication by a constant, i.e., $(1/k)$, we can do a similar procedure considering $f = \lim_{m\to \infty} f_m$, where
\begin{center}
$f_m = k \sum_{i=1}^m 1/m 1[\tilde{f}>i/m]$
\end{center}
And we get that $\mathcal{F}$ satisfies Pollard's entropy condition.
\begin{align*}
\int_0^1 \sup_{Q \in \mathcal{Q}} \sqrt{\log(N(\varepsilon ||F||_{Q,2}, Q, \mathcal{F}))} d\varepsilon &\leq \int_0^1 k \varepsilon^{-2v/(v+2)} d\varepsilon = k \int_0^1 \varepsilon^{\frac{-2v}{v+2}} d\varepsilon \\
&< +\infty
\end{align*}
because $\frac{-2v}{v+2}>-1 \iff 2v > v+2 \iff v>2$.
\end{problem}

\bigskip
%3.6
\begin{problem}
Show $\{h(w'\beta): \beta\in \mathbb{R}^d, h \in \mathcal{M}_K\}$
satisfies PEC. \\

Notice that $\mathcal{F}\equiv\{ w'\beta \equiv \sum_{i=1}^dg_i(w)\beta_i\}$ has finite
dimension, so that, by example 2, this set is VC-major. Lemma 2 then
gives that $\{h(f):f\in \mathcal{F},h\in \mathcal{M}_K\}$ is
VC-major. Therefore, since $\sup_{w\in \mathbb{R}^d}|h(f(w)))| \le K$,
($\forall f,h$), Q5 implies $\{h(f):f\in \mathcal{F},h\in \mathcal{M}_K\}$ satisfies PEC.

\end{problem}

\bigskip
%3.7
\begin{problem} Let $\mathcal{V}_K$ denote functions with total variation bounded by K. It is known that if $f \in \mathcal{V}_K$, then it can be represented as $f = g - h$ for some $g, h \in \mathcal{M}_K.$ Show that $\{h(w^\intercal \beta): \beta \in \mathbb{R}^d, h \in \mathcal{V}_K \}$ satisfies Pollard's entropy condition. This class is referred to as the Type I class in Andrew's handbook chapter. \\
	
	\textbf{Solution:} We know that $h = f - g$ for some $f, g \in \mathcal{M}_K$. From problem 6, we know that $\{f(w^\intercal \beta): \beta \in \mathbb{R}^d, h \in \mathcal{M}_K \}$ and $\{g(w^\intercal \beta): \beta \in \mathbb{R}^d, h \in \mathcal{M}_K \}$ satisfy Pollard's entropy condition.  
	
	Using Lemma 3. (with the function $-1$ and $g$) and 2. (for the functions $f$ and $-g$)from the ``Empirical Processes" notes, we can get that $\mathfrak(F) \otimes \mathfrak(-G) \equiv \{f + (-g): f \in \mathcal{M}_K and -g \in \mathcal{M}_K \}$ satisfies Pollard's entropy condition with the envelope $F + (-G)$.   

\end{problem}


\end{document}
